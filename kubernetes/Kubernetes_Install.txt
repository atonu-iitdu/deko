1) OS prep (all masters & workers)
# 1) Basic tools & time sync
sudo dnf install -y curl vim socat conntrack ebtables ethtool chrony
sudo systemctl enable --now chronyd

# 2) Disable swap (kubelet hard requirement)
sudo swapoff -a
sudo sed -ri '/\sswap\s/s/^#?/#/' /etc/fstab

# 3) Kernel modules & sysctls for Kubernetes networking
cat <<'EOF' | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter

# Kubernetes expects forwarding & bridge netfiltering
cat <<'EOF' | sudo tee /etc/sysctl.d/99-kubernetes.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF
sudo sysctl --system



2) Install & configure containerd (all masters & workers)

# Install containerd (from OS repo or docker-ce repo if needed)
sudo dnf -y install containerd || {
  sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
  sudo dnf -y install containerd.io
}

# Default config, then switch to systemd cgroups
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml >/dev/null
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl enable --now containerd


3) Install kubeadm / kubelet / kubectl (all masters & workers)

Use the official pkgs.k8s.io repo (v1.34 shown; adjust if you choose a different minor later):


sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing/SELINUX=permissive/' /etc/selinux/config

cat <<'EOF' | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.34/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.34/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF

sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
sudo systemctl enable kubelet

**These steps (including the new pkgs.k8s.io repos and the SELinux note) come straight from the official docs.


4) Open the right firewall ports

On masters (control-plane):
# Kubernetes API server
sudo firewall-cmd --permanent --add-port=6443/tcp
# etcd client API (local/peer used by other masters)
sudo firewall-cmd --permanent --add-port=2379-2380/tcp
# kubelet API
sudo firewall-cmd --permanent --add-port=10250/tcp
# kube-scheduler
sudo firewall-cmd --permanent --add-port=10259/tcp
# kube-controller-manager
sudo firewall-cmd --permanent --add-port=10257/tcp
sudo firewall-cmd --reload


On workers:

# kubelet API
sudo firewall-cmd --permanent --add-port=10250/tcp

5) Bootstrap the first control plane (kubemaster01)
kubeadm init \
  --control-plane-endpoint="192.168.122.100:6443" \
  --upload-certs \
  --apiserver-advertise-address=192.168.122.121 \
  --pod-network-cidr=192.168.0.0/16

Run that on kubemaster01. (Using a VIP for --control-plane-endpoint is the right HA pattern)

6) Install the CNI (Calico), matching your pod CIDR (192.168.0.0/16)

Operator method (recommended by Calico):
# 1) Install Calico operator & CRDs
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.30.3/manifests/tigera-operator.yaml

# 2) Create Calico default resources (includes an IP pool).
#    The default pool is 192.168.0.0/16 which matches your kubeadm --pod-network-cidr.
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.30.3/manifests/custom-resources.yaml

# 3) Watch Calico come up
watch kubectl get pods -n calico-system


(These are the current Calico quickstart commands; custom-resources.yaml can be edited to set/confirm the IP pool CIDR if you need a different range.

** Tip: If your L2 network is flat, defaults are fine. If you need VXLAN/IPIP tweaks, see Calicoâ€™s encapsulation guidance.

7) Join the other control planes (kubemaster02/03)

From a running master node run following command to generate control plane join command

# certificate key for control-plane certs
CERT_KEY=$(kubeadm init phase upload-certs --upload-certs | tail -n1)

# bootstrap token
TOKEN=$(kubeadm token create)

# discovery-token-ca-cert-hash
HASH=$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt \
  | openssl rsa -pubin -outform DER 2>/dev/null \
  | openssl dgst -sha256 -hex | awk '{print $2}')

echo "kubeadm join 192.168.122.20:6443 --token $TOKEN --discovery-token-ca-cert-hash sha256:$HASH --control-plane --certificate-key $CERT_KEY"

8) Join the workers
Again on a running master node:

kubeadm token create --print-join-command

# certificate key for control-plane certs
CERT_KEY=$(kubeadm init phase upload-certs --upload-certs | tail -n1)

# bootstrap token
TOKEN=$(kubeadm token create)

# discovery-token-ca-cert-hash
HASH=$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt \
  | openssl rsa -pubin -outform DER 2>/dev/null \
  | openssl dgst -sha256 -hex | awk '{print $2}')

echo "kubeadm join 192.168.122.20:6443 --token $TOKEN --discovery-token-ca-cert-hash sha256:$HASH --control-plane --certificate-key $CERT_KEY"